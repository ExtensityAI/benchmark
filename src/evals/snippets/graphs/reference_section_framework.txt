\section{Framework}

In this section, we discuss the design choices pertaining to implementation and the specifics of the framework we've employed.

\paragraph{Types and Representations}

Analogous to the Python \texttt{object} type, the base type of SymbolicAI is a symbol, and is represented through its name equivalent base type \texttt{Symbol}.
A \texttt{Symbol} object marks a non-reducible atomic unit.
All other sub-types, such as \texttt{Expression} and its derivatives, are analogous to their mathematical namesakes, representing expressions or units that can be further evaluated and simplified.
These sub-types inherit from \texttt{Symbol} the base attributes, primitive operators, and helper methods.
Furthermore, each \texttt{Symbol} object contains valued and vector-valued representations, obtained through \texttt{value} and \texttt{embedding} attributes.
The latter, in particular, serve as a means to impute a symbol's current context, akin to embedding text and storing it as a PyTorch tensor.
Although for an LLM, the numerical tensors may lack inherent meaning, the vector-valued representations play a strategic role: when composite symbols coalesce into more complex expressions, these embedded tensors become amenable to context updates through gradient-based optimization.
This choice is critical in the dynamic landscape of symbolic interactions, particularly for developing self-evolving systems.

To enable the processing of symbols by LLMs, we assume that each \texttt{Symbol} object is facilitated by Python's native string functionality, where the \texttt{\_\_str\_\_} method enables a string representation.
Worth noting is that encoding a complex object into a string sometimes precludes perfect object reconstitution.
However, this concern does not substantially impede our methodology: we can employ approximations or proxy representations stored by the vector-valued property to effectively re-map objects.
These representations are obtained through respective embedding models.
Therefore, we can theoretically assert that any Python object \emph{is} parsable by an LLM by design.

\paragraph{Polymorphic Context}

Polymorphism is a central concept in programming language theory and prominently featured in SymbolicAI, with significant implications for the design and functionality of our NeSy architecture.
Polymorphism refers to the ability of different objects to be accessed through the same interface, or of a single identifier to represent different types based on the context of execution.
The provision of a single interface to entities of different types allows operations to be performed in ways specific to their derived types.
We employ instruction composition in a polymorphic structure.
We designed the \texttt{Symbol} object to contain a global context, which is composed of a static and dynamic context part.
The static context is class dependent and defined at design time.
The dynamic context is runtime adaptable and can be changed to adhere to runtime specific logic and changes.
Moreover, \texttt{Symbol} associated operations resolve in a polymorphic manner before being evaluated by the NeSy engines.
SymbolicAI's engine implementation contains a \texttt{prepare} method to resolve and compile the engine specific representation by evaluating the \texttt{Symbol}-specific operations and context.
For example, when utilizing GPT-4 vision as a NeSy engine backend, we compose in the prepare statement the system and user level prompts and resolve image or video related URLs by replacing \texttt{<<vision: ... :>>} tags using \emph{regex}.

\paragraph{Aspect-Oriented Programming}

The aspect-oriented programming paradigm is a fundamental concept in modern programming language theory.
Its implementation in Python, for instance, is exemplified by the use of decorators.
Decorators offer a functional approach to extending or modifying the behavior of functions or methods without altering their code directly.
This adheres to the principles of modularity and separation of concerns, as it allows for the isolation of specific functionalities while maintaining the original function's core purpose.
By wrapping the original function with another function or a class, decorators provide an efficient and reusable way of adding or modifying behaviors. For instance, SymbolicAI integrates the zero-shot and few-shot learning with default fallback functionalities of pre-existing code.

The use of decorators brings several advantages \citep{beazley2009python, martelli2005python, summerfield2010python, lutz2013python}:

\begin{itemize}
    \item {\bf Reusability:} Decorators promote code modularity, significantly enhancing code reuse and contributing to software maintainability. This advantage is particularly salient when managing a variety of operations, reducing redundancy and easing the integration of new functionalities.
    \item {\bf Composition:} Decorators support function composition, allowing developers to construct complex functionalities from pre-existing code blocks without the need for codebase expansion or reliance on complex inheritance hierarchies.
    \item {\bf Adaptability:} Using decorators, we can easily modify or extend the behavior of operations without changing their core implementation. This flexibility facilitates the generation of adaptive workflows and reliable fallback mechanisms when experimental implementations do not fulfill required constraints.
\end{itemize}

\paragraph{Operators and Methods}
In SymbolicAI, operators are overloaded to facilitate transformations of \texttt{Symbol} objects.
These operator primitives employ dynamic casting to assure type compatibility, simplifying declarations.
Consequently, \texttt{Symbol} objects can be easily manipulated through type specific attributions or symbolically evaluated by the NeSy engine.
For example, a central operation for boolean logic is measuring equality between symbols.
For assessing the equality of symbols, we primarily adhere to type specific properties due to prioritizing strict comparisons over probabilistic assessments, and secondly to semantic equality via the NeSy engine.
As introduced earlier, SymbolicAI leverages decorators for constructing operators and custom class methods.
Upon invoking an operator or method, the respective primitive function evaluates the symbol specific type and its respective attributes, and if further necessary, resolves a nested decorated function that then references the NeSy engine for an evaluation.
Should the evaluation fail, a predefined fallback implementation executes.
Absent a fallback, or if both evaluations fail, an error state is raised.
The processing of an operator or custom method involves a pipeline consisting of pre- and post-processing steps, as well as constraint enforcement.
Constraints cover aspects like return types, value ranges, and structural integrity (e.g. JSON formatting).


\paragraph{Self-Referential Structures}
SymbolicAI augments the generative process by enabling systems to introspect and modify their behavior dynamically.
We leverage LLMs to execute tasks based on both natural and formal language instructions, adhering to the specified user objectives and with innate self-referential structures.
We derive sub-types from \texttt{Expression} and enclose their functionalities in task-specific components, which we then expose again through templating and the model-driven design of the NeSy engine.
This design choice allows the system to utilize its own sub-process definition, analogous to concepts discussed in \citep{Schmidhuber:07, Schmidhuber:09}.
Concretely, we utilize generalization properties from LLMs to interpret and formulate a set of operations that incorporate \emph{self-instructions} \citep{wang2022selfinstruct}.
Consequently, the operations hold the flexibility to adapt to the context, and derive sub-processes that self-instruct LLMs to engage in situational modeling and context-sensitive problem-solving.
Ultimately, this enables the construction of hierarchical systems and computational graphs for self-referential \emph{meta-reasoning} systems without the requirement of explicitly training a meta-learner \citep{Kirsch:22}.
