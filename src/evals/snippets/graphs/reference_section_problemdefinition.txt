\section{Problem Definition}

Conventional approaches employing foundation models for inference are predominantly confined to single-step or limited-step executions, primarily reliant on in-context learning prompt instructions of LLMs.
This restricted scope limits the utilization of LLMs' capabilities, which are constrained to single modalities, lack refinement or verification, and exhibit limited tool proficiency.
We posit that the integration of NeSy engines as core computation units, realized through logic-based methodologies coupled with sub-symbolic foundation models, offers a more general, robust, and verifiable perspective.
This approach offers several advantages.
Firstly, it facilitates the integration of pre-existing engineered solutions (e.g. various classical algorithms), offloading computational complexity and bridging various modalities.
Secondly, it enables sub-symbolic generalization to focus on evidence-based decision-making.
Thirdly, it provides an \emph{interpretable language-based control layer} for automated computing.
Central to our solution is how to define the orchestration of interactions between symbolic and sub-symbolic systems, and the level at which instructions are formulated for effective control and task execution.
