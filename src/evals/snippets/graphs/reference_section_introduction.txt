\section{Introduction}
\label{sec:introduction}

The recent surge in generative AI, particularly involving large language models (LLMs), has demonstrated their wide-ranging applicability across various domains \citep{Badita:22, Degrave:22}.
These models have enhanced the functionality of tools for search-based interactions \citep{Bing:23, You.com:22, Chatsonic:22}, program synthesis \citep{Jain:21, Parades:23, Key:23}, chat-based interactions \citep{Replika:16, OpenAIChatGPT:22, Google:23}, and many more.
Moreover, language-based approaches have fostered connections between different modalities, enabling text-to-image \citep{Ramesh:21, Saharia:22}, text-to-video \citep{Singer:22}, text-to-3D \citep{Poole:22}, text-to-audio \citep{Oord:16, Wang:17}, and text-to-code \citep{Wang:21, Lu:21, Li:22} transformations, to name a few.
Therefore, by training on vast quantities of unlabelled textual data, LLMs have been shown to not only store factual knowledge \citep{Petroni:2019,Kassner:20} and approximate usersâ€™ intentions to some extent \citep{Andreas:22}, but also to unlock deep specialist capabilities through innovative prompting techniques \citep{Nori:23}.
Yet, these applications merely scratch the surface of the transformation that language-based interactions are expected to bring to human-computer interactions in both the near and distant future.

In part, instruction-based fine-tuning of LLMs through reinforcement learning from human feedback \citep{Li:23} or direct preference optimization \citep{Rafailov:23} has shown promising results dealing with value misalignment issues \citep{Knox:08, Macglashan:17, Christiano:17, Ibarz:18, Goyal:22}, unlocking new possibilities for chain of thoughts \citep{wei:2022}, tree of thoughts \citep{Yao:23}, and graph of thoughts interactions \citep{Besta:23}.
However, recent research also highlights the limitations of LLMs in functional linguistic competence despite their proficiency in formal linguistic competence \citep{Mahowald:23}.
Whereas formal linguistic competence encompasses the ability to understand and generate language, functional linguistic competence pertains to the application of language in real-world contexts, such as conveying sensory input or recalling information from memory.
Examples of functional linguistic competence enclose implicatures \citep{Ruis:22} and contextual language comprehension beyond the statistical manifestation of data distributions \citep{Bransford:72}.
Consequently, operating LLMs through a purely inference-based approach encloses their capabilities within their provided context window, severely limiting their horizon.
This results in deficiencies for situational modeling, non-adaptability through contextual changes, and short-term problem-solving, amongst other capabilities.
However, simply increasing the context length may not yield greater capabilities, as demonstrated by the observed U-shaped performance curve \citep{Liu:23} where LLMs excel when using information at the beginning or end of the input context but struggle with information located in the middle, especially as context grows longer.
These challenges are actively being researched, with novel approaches such as Hyena \citep{Poli:23}, RWKV \citep{Bo:21}, GateLoop \citep{Katsch:23}, and Mamba \citep{Gu:23} surfacing.
Meanwhile, the re-emergence of interest in retrieval-augmented generative approaches \citep{HuayangLi:22} offers an alternative by circumventing the autoregressive nature of the widely-used Transformer architecture \citep{Vaswani:17}, enabling context enrichment with lateral information.
In parallel, efforts have focused on developing tool-based approaches \citep{Schick:23} or template frameworks \citep{Chase:22} to extend large LLMs' capabilities and enable a broader spectrum of applications. However, these efforts only partially capture the vast potential inherent in leveraging LLMs as \emph{semantic interpreters}.

Lastly, we conclude that modern programming paradigms should inherently support probabilistic computations and provide a native and boilerplate-free set of features for constructing computational graphs. This includes composition, inheritance, parallel and simulation-based executions, polymorphic operators, and self-referential structures. Current programming languages often have disjointed, bloated, or makeshift solutions for these concepts. We believe that holistically integrating probabilistic support for these concepts into modern software and hardware will unlock new programming capabilities that take advantage of probabilistic architectures. We hope the community will consider these ideas as essential components of contemporary computing.
We also wish to express our concern about the current trends in deep technology, where there is a significant concentration of data and resources, coupled with a tendency towards closed-source practices. We strongly advocate for increased transparency and the sharing of ideas to ensure diverse and collective growth in our socio-economic landscape.

In summary, we highlight the following key contributions of this work:
\begin{itemize}
    \item We introduce SymbolicAI, a logic-based framework for concept learning and flow management in generative processes, enabling seamless integration with a wide range of foundation models and solvers.
    \item Additionally, we augment solvers with LLM-based generalizability, and facilitate the human-computer interactions for automated computational problem-solving.
    \item Further, we introduce a quality measure and benchmark to showcase the potential of composition, inheritance, multi-modality, multi-agent interactions, self-referential structures, and automated sub-routines.
    \item Lastly, we advocate and lead directions for the future development of dedicated programming languages that promotes native support of language-centric probabilistic approaches.
\end{itemize}
