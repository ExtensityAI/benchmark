We present \emph{SymbolicAI}, a highly adaptive and modularized architecture that takes a logical stance on concept learning and the orchestration of generative processes. This architecture ensures effortless fusing of generative architectures with a broad spectrum of solver technologies. By amalgamating sizable language models (LLMs) and principles of probabilistic programming, we handle sophisticated challenges and harness the respective powerhouses of differentiable and traditional programming regimes. At the heart of our architecture lie neuro-symbolic engines, constituting LLMs responsible for the execution of tasks penned in both natural and formal languages. Our methodology crafts a suite of operations that sculpt the data stream, effectively nudging the LLMs towards fulfilling the aspirations of the users. These operations are inherently polymorphic, compositional, and self-referential, empowering us to flexibly incorporate a multitude of data types and behaviors. Such flexibility allows for smooth transitions between the skill sets of various foundational models, all equipped with zero and few-shot learning proficiencies, as well as specialized models or solvers meticulously devised to take on particular challenges. Consequently, our architecture paves the way for the genesis of intelligible computational graphs through the assembly of compositional expressions and functions.