import os
import argparse
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from src.func import run


BENCHMARK_NAME_MAPPING = {
    'eval_in_context_associations': 'Associations',
    'eval_multimodal_bindings': 'Modality',
    'eval_program_synthesis': 'Code',
    'eval_components': 'Components',
    'eval_computation_graphs': 'Graphs'
}


MODEL_NAME_MAPPING = {
    'gpt4': 'GPT-4 Turbo',
    'gpt3.5': 'GPT-3.5 Turbo',
    'gemini': 'Gemini-Pro',
    'llama': 'LlaMA 2 13B',
    'mistral': 'Mistral 7B',
    'zephyr': 'Zephyr 7B'
}


DUMMY_DATA = {
    f"{MODEL_NAME_MAPPING['gpt3.5']}": {
        f"{BENCHMARK_NAME_MAPPING['eval_in_context_associations']}": {"performance": 0.8},
        f"{BENCHMARK_NAME_MAPPING['eval_multimodal_bindings']}": {"performance": 0.6},
        f"{BENCHMARK_NAME_MAPPING['eval_program_synthesis']}": {"performance": 0.7},
        f"{BENCHMARK_NAME_MAPPING['eval_components']}": {"performance": 0.67},
        f"{BENCHMARK_NAME_MAPPING['eval_computation_graphs']}": {"performance": 0.7}
    },
    f"{MODEL_NAME_MAPPING['gpt4']}": {
        f"{BENCHMARK_NAME_MAPPING['eval_in_context_associations']}": {"performance": 0.95},
        f"{BENCHMARK_NAME_MAPPING['eval_multimodal_bindings']}": {"performance": 0.85},
        f"{BENCHMARK_NAME_MAPPING['eval_program_synthesis']}": {"performance": 0.8},
        f"{BENCHMARK_NAME_MAPPING['eval_components']}": {"performance": 0.85},
        f"{BENCHMARK_NAME_MAPPING['eval_computation_graphs']}": {"performance": 0.9}
    },
    f"{MODEL_NAME_MAPPING['gemini']}": {
        f"{BENCHMARK_NAME_MAPPING['eval_in_context_associations']}": {"performance": 0.9},
        f"{BENCHMARK_NAME_MAPPING['eval_multimodal_bindings']}": {"performance": 0.89},
        f"{BENCHMARK_NAME_MAPPING['eval_program_synthesis']}": {"performance": 0.78},
        f"{BENCHMARK_NAME_MAPPING['eval_components']}": {"performance": 0.75},
        f"{BENCHMARK_NAME_MAPPING['eval_computation_graphs']}": {"performance": 0.7}
    },
    f"{MODEL_NAME_MAPPING['llama']}": {
        f"{BENCHMARK_NAME_MAPPING['eval_in_context_associations']}": {"performance": 0.6},
        f"{BENCHMARK_NAME_MAPPING['eval_multimodal_bindings']}": {"performance": 0.45},
        f"{BENCHMARK_NAME_MAPPING['eval_program_synthesis']}": {"performance": 0.4},
        f"{BENCHMARK_NAME_MAPPING['eval_components']}": {"performance": 0.3},
        f"{BENCHMARK_NAME_MAPPING['eval_computation_graphs']}": {"performance": 0.45}
    },
    f"{MODEL_NAME_MAPPING['mistral']}": {
        f"{BENCHMARK_NAME_MAPPING['eval_in_context_associations']}": {"performance": 0.67},
        f"{BENCHMARK_NAME_MAPPING['eval_multimodal_bindings']}": {"performance": 0.5},
        f"{BENCHMARK_NAME_MAPPING['eval_program_synthesis']}": {"performance": 0.45},
        f"{BENCHMARK_NAME_MAPPING['eval_components']}": {"performance": 0.4},
        f"{BENCHMARK_NAME_MAPPING['eval_computation_graphs']}": {"performance": 0.3}
    },
    f"{MODEL_NAME_MAPPING['zephyr']}": {
        f"{BENCHMARK_NAME_MAPPING['eval_in_context_associations']}": {"performance": 0.7},
        f"{BENCHMARK_NAME_MAPPING['eval_multimodal_bindings']}": {"performance": 0.6},
        f"{BENCHMARK_NAME_MAPPING['eval_program_synthesis']}": {"performance": 0.5},
        f"{BENCHMARK_NAME_MAPPING['eval_components']}": {"performance": 0.43},
        f"{BENCHMARK_NAME_MAPPING['eval_computation_graphs']}": {"performance": 0.36}
    }
}



def parse_args():
    parser = argparse.ArgumentParser(description='Run the benchmark.')
    parser.add_argument('--context_associations', action='store_true', help='Run the in-context associations benchmark.')
    parser.add_argument('--multimodal_bindings', action='store_true', help='Run the multimodal bindings benchmark.')
    parser.add_argument('--program_synthesis', action='store_true', help='Run the program synthesis benchmark.')
    parser.add_argument('--components', action='store_true', help='Run the components benchmark.')
    parser.add_argument('--computation_graphs', action='store_true', help='Run the computation graphs benchmark.')
    parser.add_argument('--all', action='store_true', help='Run all benchmarks.')
    parser.add_argument('--dummy', action='store_true', help='Run the dummy benchmark.')
    return parser.parse_args()


LATEX_TEMPLATE = """
\\begin{{figure*}}[ht]
  \\centering
  \\begin{{minipage}}{{.57\\textwidth}}
    \\centering
    \\begin{{tabular}}{{lcccccc}}
      \\toprule
      \\textbf{{Benchmarks}} &  {model_names} \\\\
      \\midrule
      {benchmark_in_context_association_row} \\\\
      {benchmark_multimodality_row} \\\\
      {benchmark_program_synthesis_row} \\\\
      {benchmark_components_row} \\\\
      {benchmark_computation_graphs_row} \\\\
      \\midrule
      \\textbf{{Total}} & {total_row} \\\\
      \\bottomrule
    \\end{{tabular}}
    % You could also add a subcaption specific to the table here if needed (use the subcaption package)
    % \\subcaption{{Performance benchmark results.}}
    \\label{{tab:benchmark_results}}
  \\end{{minipage}}%
  ~
  \\begin{{minipage}}{{.43\\textwidth}}
    \\centering
    \\includegraphics[width=\\linewidth]{{images/benchmark_comparison_chart.pdf}}
    % You could also add a subcaption specific to the figure here if needed (use the subcaption package)
    % \\subcaption{{Benchmark comparison chart.}}
    \\label{{fig:spider_plot}}
  \\end{{minipage}}
  \\caption{{Placeholder for performance benchmarks and comparison chart for various models.}}
  \\label{{fig:my_label}} % General label for the whole figure (both image and table)
\\end{{figure*}}
"""


def create_latex_result(data):
    latex_table = ''
    # TODO: Write this function to create a latex table from the data

    return latex_table


if __name__ == '__main__':
    args = parse_args()
    results = run(args)
    create_latex_result(DUMMY_DATA)

